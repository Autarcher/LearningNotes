# 一、文件系统

## 日志机制

文件系统中的一致性通常由日志机制实现。

日志的核心主要包括两个方面，文件读写落盘之前日志先落盘，记录日志的checkpoint。

利用日志的checkpoint和文件系统的undo，redo机制保证文件的一致性。

## 1.分布式文件系统

> 分布式文件系统的master通常都是使用心跳机制判断node是否存活。

### 1.0 拓展与设计理念

####  1.0.1 可能的故障原因:

​	（BigTable）我们学到的一个教训是，**大型分布式系统容易受到许多类型的故障的影响**，而不仅仅是许多分布式协议中假设的标准网络分区和故障停止故障。例如，我们已经看到了由于以下所有原因造成的问题：内存和网络损坏，大时钟偏差，挂起的机器，扩展和不对称的网络分区，我们正在使用的其他系统中的错误（例如Chancellor）、GFS配额溢出以及计划内和计划外的硬件维护。

#### 1.0.2 推迟添加新功能

​	（BigTable）我们学到的另一个教训是，**推迟添加新功能是很重要的，直到清楚了新功能将如何使用。**例如，我们最初计划在我们的API中支持通用事务。因为我们没有立即使用它们，但是，我们没有实现它们。现在我们有许多真实的应用程序在Bigtable上运行，我们已经能够检查它们的实际需求，我们发现，大多数应用程序只需要单行事务。在人们请求分布式事务的地方，最重要的用途是维护二级索引，我们计划添加一个专门的机制来满足这一需求。新机制的通用性将低于分布式事务，而且会更高效（特别是对于跨越数百行或更多行的更新），并且还将与我们的乐观跨数据中心复制方案更好地交互

#### 1.0.3 适当的系统监控

​	我们从支持Bigtable中学到的一个实际经验是**适当的系统级监控的重要性**（即，监控Bigtable本身以及使用Bigtable的客户端进程）。例如，我们扩展了RPC系统，以便对于RPC示例，它会详细跟踪代表该RPC执行的重要操作。

#### 1.0.4 简单的设计最重要

​	我们学到的最重要的一课是简单设计的价值。（大约100，000行非测试代码），以及代码以意想不到的方式随着时间的推移而演变的事实，**我们发现代码和设计的清晰度对代码维护和调试有巨大的帮助。**这方面的一个例子是我们的tablet-server成员资格协议。我们的第一个协议很简单：主服务器定期向tablet服务器发出租约，如果租约到期，tablet服务器会自行终止。不幸的是，该协议在出现网络问题时会显著降低可用性，并且对主服务器恢复时间也很敏感。我们多次重新设计该协议，直到有一个性能良好的协议。然而，最终的协议太复杂了，依赖于其他应用程序很少使用的Champion特性的行为。我们发现，我们花费了过多的时间来调试晦涩的角落情况，不仅在Bigtable代码中，最后，我们放弃了这个协议，转而使用一个更新的更简单的协议，它只依赖于广泛使用的Champion特性

#### 1.0.5 数据存储

​	参考RAID的数据条带化存储。文件系统也可以这样，下层将文件差分到不通的节点中。

#### 1.0.6 中心服务器

​	对文件空间的操作（增删查改），由中心控制，由存储节点负载正在的I/O。

### 1.1 GFS: 一个面向超大文件的分布式文件系统 2003

> 原文The Google File System

![image-20250416173608737](C:\Users\lya\Desktop\lya\image\image-20250416173608737.png)

1.大部分文件都是只会在文件尾新增加数据，而少见修改已有数据的。对一个文件的随机写操作在实际上
几乎是不存在的。

>  Third, most files are mutated by appending new data rather than overwriting existing data. （The Google File System）

2.GFS的约定:对于读来说，文件系统中存在多数操作是大的流式读和小的随机读。

3.对于超大的文件的分布式文件系统而言，客户端 cache 机制没啥用处，这是因为大部分的应用都是
流式访问超大文件或者操作的数据集太大而不能被 cache。

4.分布式系统的时间备份传输是基于TCP，全双工通道的流水线传输方式。

一次对文件操作的流

![image-20250417103932758](C:\Users\lya\Desktop\lya\image\image-20250417103932758.png)

![image-20250417103942107](C:\Users\lya\Desktop\lya\image\image-20250417103942107.png)

#### 1.1.1数据如何保证一致性：

​	利用写入数据的校验信息标识唯一性，哪怕重复提交，数据使用方也可以通过唯一标识符识别重复内容。

#### 1.1.2快照

​	快照采用，写时复制实现内存利用

#### 1.1.3锁的延迟分配

​	锁在使用的时候才创建，不适用的时候就会被删除。

#### 1.1.4文件的删除

​	 通过重命名实现删除的简介与效率。同时提供多种删除模式给应用程序满足应用程序的需求。

> 当一个文件被应用程序删除时，master会像其他更改一样立即记录删除。但是，master不会立即回收资源，而是将文件重命名为包含删除时间戳的隐藏名称。在master对文件系统命名空间的常规扫描期间，如果任何隐藏文件存在超过三天（间隔可配置），它会删除这些文件。在此之前，仍然可以使用新的特殊名称读取该文件，并可以通过将其重命名为正常名称来取消删除。当隐藏文件从命名空间中删除时，它在内存中的元数据也会被删除，这有效地切断了它与所有块的链接。



### 1.2 GPFS：分布式文件系统2002

> GPFS: A Shared-Disk File System for Large Computing Clusters

#### 1.2.1大型目录的支持(数百万个文件)

核心思想：使用 **可扩展哈希（Extensible Hashing）** 优化目录查询，

> 容量问题：
>
> 注意虽然理论上 n 可以任意增长，但现实中：
>
> - **块编号最大值受限于文件系统实现的 block 数量上限**（比如 32 位块号最多支持 4G 块）
> - **目录文件的最大大小也会成为一个瓶颈**
> - **系统内存、磁盘 IO、元数据管理成本**也会随着增长而增加
>
> 但在实践中，即使是几百万文件，使用 20～24 位哈希空间就足够用了，不容易遇到“用完哈希”的情况。
>
> 删除问题:
>
> ​	苦于类似内存管理的伙伴系统一样回收，但是GPFS是采取的是延迟合并清除不立即施放内存块。(类似于定期扫描)

📌 目标问题：

在拥有**百万级文件数量**的大型目录中，**文件名查找**若仍使用线性遍历，会极大影响性能。

🧩 解决方法：**可扩展哈希技术**

GPFS 使用一种**按需分裂的哈希表结构**，即「**可扩展哈希（Extensible Hashing）**」，它的特点是：

1. **每个目录文件是一个稀疏文件（sparse file）**，每个目录项会根据哈希值放入特定的 block 中。
2. **哈希函数**将文件名映射成一个值，然后根据该值的低位若干比特（n bits）确定其应存放的块编号。
3. 如果一个 block 装满了，会**按位“分裂”成两个块**，并根据哈希值的第 `n+1` 位重新分配已有数据。
4. 这个分裂机制只影响一个块，其他块不变，实现了**局部动态增长**。

------

✅ 举个例子：

假设我们有一个空目录开始往里面放文件。

🔢 初始状态：

- 当前目录只有一个块（block 0）。
- 使用哈希函数的 **最低1位** 来选择块编号（即 `n=1`）。
- 所有文件哈希值的最低1位都为 `0` 或 `1`，但是现在只有 block 0 存在，所以全都放到 block 0。

------

🆕 添加新文件时，block 0 满了：

- 系统执行“分裂”操作，将 block 0 拆分为：
  - 原始的 block 0
  - 新的 block 1
- 现在使用哈希值的 **前 2 位（n=2）**，判断条目属于哪个块：
  - 如果哈希值的前两位是 `00` → 仍然留在 block 0
  - 如果是 `10` → 移动到 block 1（即低1位是0，但次低1位是1）

------

📁 文件名查找过程：

- 想查找文件名 `"hello.txt"`：
  1. 先对 `"hello.txt"` 做哈希，得到一个值，比如 `0b10101010...`
  2. 根据目录的当前分裂深度（比如 `n=3`），取哈希值的低 3 位（假设是 `101`），就知道要去第 5 号块（即 block 5）找。
  3. 因为每次插入时都只改变一个块的位置，其他不变，所以**不需要遍历所有目录块**，一次 block 访问就够了。

------

🌌 目录文件是稀疏文件（Sparse File）

- 分裂不是一次性完成的，是**按需进行**，所以目录文件中很多 block 是“洞”（hole），即尚未分裂的部分没有对应的块。
- 系统通过这些“洞”的位置，可以推断出当前使用了多少位的哈希值（即 `n` 的大小）。

------

🧾 小结



| 特性     | 描述                                     |
| -------- | ---------------------------------------- |
| 哈希方式 | 使用扩展哈希，按需增加目录块             |
| 分裂机制 | 按需分裂某个满的块，只移动该块内容       |
| 查找效率 | 每次查找只需访问**一个块**               |
| 文件结构 | 目录是稀疏文件，未分裂块以“洞”的形式存在 |

#### 1.2.2集群的一致性，并行性保证

##### 1.2.2.1 一致性	

​	通过文件锁的机制实现，利用锁来保证文件操作的顺序和文件系统的一致性。锁采用一个全局的锁管理器和本地锁管理器结合的方式。

##### 1.2.2.2 并行性

​	通过文件的字节,保证写入的并行化和一致性。

#### 1.2.3读取文件目录的性能问题

> 我感觉这个处理有点简陋且容易错误的占用cpu。

​	**多线程预读：**早期版本的 GPFS 在执行 “ls –l” 和增量备份等程序时存在严重的性能问题，这些程序对目录中的每个文件调用 stat（）。stat（） 调用读取文件的 inode，这需要读取令牌。如果另一个节点持有此令牌，则释放它可能需要在该节点上写回脏数据，因此获取令牌的成本可能很高。我们通过利用并行性解决了这个问题。当 GPFS 检测到对同一目录中的 inode 的多次访问时，它会使用多个线程为同一目录中的其他文件预取 inode。inode 预取将目录扫描速度提高了近 10 倍。

### 1.3 Calypso文件系统：分布式文件系统中的令牌与锁

> 原文 Distributed Token Management in Calypso File System （基于Calypso文件系统）

![image-20250425162017577](C:\Users\lya\Desktop\存储和文件系统相关的论文\image\image-20250425162017577.png)

LLM: Local Lock Manager

MCS: Lock Control Server

#### 1.3.1 并发修改文件时文件的stat属性修改

​	stat的修改和文件内容的修改独立上锁（单独的接口，主要时针对文件的修改时间属性的修改），以防stat属性的修改成为文件并发写入的瓶颈。



#### 1.3.2 令牌的仲裁流程方式

- 以LCS服务器为中心

  以服务器端为中心的申请令牌流程如下：

  1. **客户端发送请求**：本地锁管理器（LLM）若要授予锁，需先拥有相应令牌。当 LLM 没有所需令牌时，会向锁控制服务器（LCS）发送获取令牌的请求。

  2. **服务器检查并决策**：LCS 收到请求后，会查看自身存储令牌状态信息的数据结构 LCS - table。依据令牌类型的兼容性定义，判断是否能将令牌授予请求的 LLM。

  3. **处理令牌冲突**：如果令牌不能被授予，这意味着存在其他 LLM 持有冲突的令牌。此时，LCS 会向持有冲突令牌的其他 LLM 发送消息，强制它们要么撤销（等同于使令牌无效）令牌，要么将令牌降级（放弃独占模式）。

  4. **授予令牌**：当 LCS 确认相关令牌已被撤销或降级，解决了令牌冲突问题后，便会将令牌授予最初请求的 LLM。

     > 特点
     >
     > 该流程中，服务器在整个令牌仲裁过程中起到核心控制作用，负责检查、决策和协调。虽然这种方式***安全性较高***，恶意客户端难以扰乱系统，但也存在明显缺点。一方面，所有工作都由服务器完成，会使***服务器负载过重***，导致协议的可扩展性降低；另一方面，为避免死锁，执行撤销 / 降级操作的线程与接收新请求的线程必须分开，且要实现***两组线程间的通信***，这使得***管理两组通信进程变得复杂* 。**

- 以LLM客户端为中心

  - 工作流程

    1. **客户端请求令牌**：当本地锁管理器（LLM）没有所需令牌时，会向锁控制服务器（LCS）发送消息，请求以特定模式获取令牌。例如，某客户端的 LLM 想要获取一个文件数据页的读令牌，就会向 LCS 发送这样的请求。

    2. **服务器回应**：LCS 接收到请求后，会根据自身存储令牌状态信息的 LCS-table，判断是否能将令牌授予该 LLM。如果可以授予，LCS 会直接把令牌授予请求的 LLM，该 LLM 收到令牌后更新 LLM-table，并将锁授予请求进程；若无法授予，LCS 会返回一个包含持有冲突模式令牌节点的列表（copyset）。比如，若有其他客户端的 LLM 持有该文件数据页的写令牌，LCS 就无法直接授予读令牌，会返回这个写令牌持有者的节点信息。

    3. **客户端协调**：请求令牌的 LLM 收到包含冲突节点的列表后，会联系这些持有冲突模式令牌的 LLM，要求它们要么将令牌降级（如从写模式降级为读模式 ），要么撤销令牌。例如，请求读令牌的 LLM 联系持有写令牌的 LLM，协商令牌的降级或撤销。

    4. **完成令牌获取**：当请求令牌的 LLM 成功使其他 LLM 完成令牌的降级或撤销后，会通知 LCS 更新 LCS-table，并请求 LCS 将令牌授予自己。LCS 确认相关操作完成后，就会把令牌授予该 LLM。

       > - **优势**：客户端中心方案最大的优势是***减轻了服务器的负载***。在传统的服务器中心方案中，服务器要承担处理令牌仲裁的所有工作，负载压力大；提升了***系统的可扩展性***。在有大量客户端请求令牌的情况下，服务器中心方案可能会因服务器负载过高而性能下降，客户端中心方案则能更好地应对这种情况。
       > - **劣势与应对措施**：该方案的主要劣势是对***客户端的信任要求较高***，因为客户端要执行对协议正确性至关重要的操作。不过在 Calypso 文件系统所在的集群环境中，所有节点属于同一管理域，这个问题的影响较小。此外，***对于无法立即满足的令牌请求，客户端中心方案会额外产生两条消息（一条发送消息和一条回复消息）***，用于通知 LCS 客户端已成功撤销或降级令牌。不过，可以通过优化，让 LCS 假设令牌最终会被请求者撤销或降级并直接更改状态表，以此减少消息数量，但这种优化需要在恢复方案中处理由此增加的不确定性 
       >
       > - **举例**：假设有三个客户端节点 A、B、C，节点 A 的 LLM 想要获取某个文件数据页的写令牌，而此时节点 B 的 LLM 持有该数据页的读令牌。节点 A 的 LLM 向 LCS 发送写令牌请求，LCS 根据 LCS-table 发现节点 B 持有冲突的读令牌，无法直接授予，于是返回包含节点 B 的 copyset 给节点 A。节点 A 的 LLM 收到后，联系节点 B 的 LLM，请求其撤销或降级读令牌。节点 B 的 LLM 同意撤销读令牌后，节点 A 的 LLM 通知 LCS 更新 LCS-table，并请求授予写令牌，LCS 确认后将写令牌授予节点 A 的 LLM。

#### 1.3.3 避免死锁的方法

​	Calypso文件系统采用：按顺序申请资源。

------



#### 1.3.4 令牌的状态迁移 --一致性保证

![image-20250428142021745](C:\Users\lya\Desktop\存储和文件系统相关的论文\image\image-20250428142021745.png)

图 5 展示了实现多读者 / 单写者语义的令牌状态转移图，用于描述 Calypso 文件系统中令牌在不同事件触发下的状态变化，具体状态迁移如下：

1. **初始状态（INVALID）**：最初，所有令牌在客户端都处于 INVALID（无效）状态。此时客户端尚未对令牌进行任何操作，没有进程持有该令牌的有效访问权限。
2. **本地请求（从 INVALID 到 ACQUIRING）**：当客户端发起本地请求获取令牌时，令牌状态从 INVALID 转变为 ACQUIRING（获取中）。这表示客户端正在尝试获取令牌，处于获取令牌的过程中。
3. **请求成功（从 ACQUIRING 到 READ 或 WRITE）**：若请求成功完成，令牌会根据请求的类型进入不同状态。如果是读请求，令牌状态变为 READ（读），表示当前客户端可以进行读操作，并且可以有多个客户端同时处于读状态；如果是写请求，令牌状态变为 WRITE（写），此时只有该客户端能进行写操作，其他客户端不能获取该令牌的任何模式访问权限。
4. **网络写事件（从 READ 到 INVALID）**：当令牌处于 READ 状态时，如果接收到网络写事件，即其他客户端发起了写操作请求，那么当前令牌状态会变为 INVALID。这是因为写操作会改变数据，为保证数据一致性，正在读的令牌需要变为无效状态，后续若要再次访问数据，需要重新获取令牌。
5. **其他可能的状态迁移**：从 WRITE 状态若有其他客户端请求写操作，原持有写令牌的客户端释放令牌后，令牌可能回到 INVALID 状态；处于 ACQUIRING 状态时，如果获取令牌失败（例如有其他客户端持有冲突模式的令牌且无法协调），也可能回到 INVALID 状态等待后续重新请求 。

这些状态迁移共同构成了一个复杂的状态转换系统，确保在多读者 / 单写者语义下，令牌能正确控制对文件资源的访问，保证文件系统的一致性和正确性。但由于状态迁移情况复杂，若没有形式化方法和工具辅助，很难验证协议的正确性。



#### 1.3.5 错误恢复和容错

**在 Calypso 文件系统中**：NSS（Node Status Service）运行在每个节点上。它利用心跳机制和仲裁共识，跟踪集群内每个节点的状态，判断节点是处于正常运行还是故障状态。当某个节点发生故障时，NSS 能迅速察觉并将各类故障统一映射为节点故障，随后通过选举产生的领导者节点启动恢复流程，以保障整个文件系统在面对节点故障时仍能维持稳定运行 。在集群环境下，每个节点的 NSS 相互协作，共同维护集群状态的一致性和稳定性，为文件系统的正常运作提供基础支持。

#### 1.3.6.（拓展） 死锁

​	**一、死锁的必要条件（Coffman 条件）**

1. **互斥条件（Mutual Exclusion）**

2. **占有并等待条件（Hold and Wait）**
   
3. **不可抢占条件（No Preemption）**
   
4. **循环等待条件（Circular Wait）**

**二、死锁预防（Preventing Deadlock）**

通过破坏死锁的四个必要条件之一来避免死锁的发生：

1. **破坏互斥条件**

   - **方法**：将独占资源改为共享资源。
   - **示例**：允许多个进程同时访问只读文件。
   - **局限性**：并非所有资源都可共享（如打印机、锁）。

2. **破坏占有并等待条件**

   - 方法：

     - **一次性分配**：进程在运行前一次性申请所有需要的资源。
  - **逐步分配**：仅当进程不持有任何资源时才允许其申请资源。
    
   - **示例**：线程启动前获取所有必需的锁。

   - **局限性**：可能导致资源浪费（提前占用但暂时不用）。

3. **破坏不可抢占条件**

   - 方法：

     - 允许进程在无法获取新资源时释放已占有的资源。
  - 操作系统支持资源抢占（如高优先级进程可抢占低优先级进程的资源）。
    
   - **示例**：数据库事务超时回滚。

   - **局限性**：某些资源的抢占可能导致数据不一致（如文件操作）。

4. **破坏循环等待条件**

   - 方法：

     - **资源排序**：为资源分配全局序号，进程必须按序号递增的顺序请求资源。

   - **示例**：若锁 L1 序号小于 L2，则线程必须先获取 L1 再获取 L2。

   - **局限性**：资源序号难以动态维护，可能影响程序设计。

**三、死锁解除（Breaking Deadlock）**

当死锁已经发生时，通过以下方法解除死锁：

1. **进程终止（Process Termination）**

   - **方法**：强制终止一个或多个死锁进程，释放其占用的资源。

   - 策略：

     - **终止所有死锁进程**：简单但代价高。
  - **逐步终止**：按某种顺序（如优先级、资源占用量）逐个终止进程，直到死锁解除。
    
   - **示例**：操作系统终止无响应的应用程序。

2. **资源抢占（Resource Preemption）**

   - **方法**：从一个或多个进程中抢占资源并分配给其他进程。

   - 策略：

     - **回滚（Rollback）**：将进程回滚到安全状态后重新执行。
  - **检查点（Checkpointing）**：定期保存进程状态，以便回滚。
    
   - **示例**：虚拟内存中的页面置换。

3. **资源分配图算法**

   - **方法**：通过检测资源分配图中的环来发现死锁，并选择合适的进程进行终止或资源抢占。
   - **局限性**：对大规模系统的计算开销较大。

4. **鸵鸟算法（Ostrich Algorithm）**

   - **方法**：忽略死锁，仅在死锁发生频率较低且影响较小时使用。
   - **适用场景**：如个人计算机中的应用程序死锁，用户可手动重启。

**四、死锁避免（Avoidance） vs 死锁检测（Detection）**

- **死锁避免**（如银行家算法）：通过动态评估资源请求的安全性，确保系统不会进入可能导致死锁的状态。

- **死锁检测**：允许系统进入死锁状态，但通过算法定期检测并解除死锁。



### 1.4 AFS 分布式文件系统 1988

### 1.5 Lustre 分布式文件系统2003

> 文件的数据对象还是采用RAID模式的条带化存储分布再多个OST上

​	三层思想 MDS（Metadata Servers）管理控制层, OST（Object Storage Targets）服务层, OBDs（Object-Based Disks）存储层。

![image-20250507165535266](C:\Users\lya\Desktop\lya\LearningNotes\image\image-20250507165535266.png)

#### 1.6.1 创建文件

​	类似与GFS的中心服务器管理，所有节点的创建由MDSs(metadata Servers )服务器接受和管理。

### 1.6 GlusterFS 分布式文件系统2005

### 1.7 Ceph 分布式文件系统2006

> 学习资料:https://docs.gluster.org/en/main/Quick-Start-Guide/Architecture/

### 1.8 RADOS 分布式文件系统2007

### 1.9. HDFS 分布式文件系统2010












------

  



# 二、存储系统

## 1.RAID

阿里社区：https://developer.aliyun.com/article/586759

掘金社区：https://juejin.cn/post/7085945428507623432

CSDN:https://blog.csdn.net/ChenVast/article/details/77906712

## 2.BigTable: 分布式存储系统 

> Bigtable: A Distributed Storage System for Structured Data

![image-20250428171550381](C:\Users\lya\Desktop\存储和文件系统相关的论文\image\image-20250428171550381.png)

### 2.1 局部性原理

​	在Webtable中，通过反转URL的主机名组件，将同一域中的页面分组到相邻的行中。例如，我们将www.example.com的数据存储maps.google.com/index.html在关键字com.google. maps/index.html下。将同一域中的页面存储在彼此附近可以使某些主机和域分析更有效。

### 2.2 文件压缩

​	Bigtable ***支持多种压缩格式***，如 Snappy、Zippy 和 Gzip 等。这些压缩格式各有特点，例如 Snappy 压缩速度快，适用于对压缩和解压缩速度要求较高的场景；Gzip 压缩比高，能显著减少数据存储空间，但压缩和解压缩的速度相对较慢。

​	数据压缩在 Bigtable 中有多方面的好处。一方面，它可以减少数据在磁盘上的存储占用空间。例如，在一个存储大量用户日志数据的 Bigtable 实例中，通过压缩可以将原本占用大量磁盘空间的数据量大幅减少，从而降低存储成本。另一方面，压缩后的数据在网络传输和磁盘读写时，由于数据量变小，能够提高传输和读写的效率。比如，当需要从磁盘读取数据并通过网络传输给客户端时，压缩后的数据可以更快地传输，减少用户等待时间。

# 三、数据库

## 1.并行数据库

> DEWITT, D. J., AND GRAY, J. Parallel database systems: The future of high performance database systems.
>
> CACM 35, 6 (June 1992), 85–98

## 2.主存数据库

> DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L.,
> STONEBRAKER, M., AND WOOD, D. Implementation
> techniques for main memory database systems. In Proc.
> of SIGMOD (June 1984), pp. 1–8

# 四、拓展内容

## 1.MapReduce

> MapReduce: Simplified Data Processing on Large Clusters

![image-20250428161117641](C:\Users\lya\Desktop\存储和文件系统相关的论文\image\image-20250428161117641.png)

### 1.1 拖尾问题的处理

1. **“拖后腿任务” 的产生原因**：在 MapReduce 操作中，“拖后腿任务” 指那些花费异常长时间来完成计算中最后几个 map 或 reduce 任务的情况。其产生原因多种多样。比如，机器磁盘存在故障，可能频繁出现可纠正错误，使磁盘读取性能从 30MB/s 大幅降至 1MB/s；集群调度系统在同一机器上安排了其他任务，导致该机器上运行的 MapReduce 代码因 CPU、内存、本地磁盘或网络带宽竞争而执行缓慢；还有可能是机器初始化代码存在漏洞，像导致处理器缓存被禁用，使受影响机器上的计算速度减慢超百倍。
2. **备份任务机制**：为缓解 “拖后腿任务” 的影响，MapReduce 采用备份任务机制。当 MapReduce 操作接近完成时，主节点会为剩余仍在进行中的任务安排备份执行。只要主任务或备份任务中有一个完成，该任务就会被标记为完成。这种机制在性能优化方面效果显著，经过调整，它通常只会使操作所使用的计算资源增加几个百分点，但能大幅减少大型 MapReduce 操作的完成时间。以文档中的排序程序为例，当禁用备份任务机制时，其完成时间会延长 44% 。

## 2.文件压缩算法

### 2.1**压缩算法对比表**

| 算法       | 压缩率 (%) | 压缩速度 (MB/s) | 解压缩速度 (MB/s) | 典型应用场景        | 开源许可           | 商业友好 | 官方维护   |
| ---------- | ---------- | --------------- | ----------------- | ------------------- | ------------------ | -------- | ---------- |
| **gzip**   | 35-60      | 100-200         | 300-500           | HTTP 压缩、文件归档 | GPLv2              | ❌        | GNU        |
| **Snappy** | 20-50      | 250-500         | 500-1000          | 实时数据传输        | BSD 3-Clause       | ✅        | Google     |
| **zstd**   | 40-70      | 100-300         | 400-800           | 综合性能优化        | BSD + GPLv2 双许可 | ✅        | Facebook   |
| **bzip2**  | 45-75      | 20-50           | 100-200           | 归档存储            | BSD-like License   | ✅        | 独立开发者 |

关键说明：

1. **商业友好**：指是否允许在闭源商业软件中使用。

2. **压缩率**：数值越高表示压缩后体积越小。

3. **速度**：单位为 MB/s，数值越高表示性能越好。

4. 开源许可

   ：

   - **GPLv2**：基于该算法的衍生代码需开源。
   - **BSD 系列**：允许自由使用、修改和闭源分发。

根据具体场景选择算法时，建议优先考虑**压缩率**与**速度**的平衡，以及**开源许可**的限制。

### 2.2 注意：

​	**对同一个文件反复执行压缩（即对已经压缩过的文件再次压缩），**像 Snappy 这类压缩算法 **不会让文件变得更小，反而通常会变大或效果不变**。

### ✅ 为什么再次压缩没有意义？

1. **压缩算法的本质是去冗余**

- Snappy 是一种**无损压缩算法**，其目标是发现数据中的**重复模式**并以更短的方式表示；
- 第一次压缩时，数据中的冗余性已被“挤压”干净；
- 第二次压缩面对的是**结构化后的数据**，而不是自然冗余的数据，所以：
  - 冗余性已经很低；
  - 新的压缩器无法找到有效模式；

2. **压缩后的数据更像“随机比特”**

- 高质量压缩后的数据**熵值很高**（即信息密度很高）；
- 随机数据压缩几乎没有效果（没有重复）；
- 对压缩后的数据再压缩，几乎没有压缩空间，甚至会增加元信息 → 文件更大。

| 再次压缩                               | 是否有意义   | 说明                                    |
| -------------------------------------- | ------------ | --------------------------------------- |
| 用 **Snappy 压缩 Snappy 输出**         | ❌ 没有意义   | 几乎不会再变小，甚至变大                |
| 用 **不同压缩器组合使用**              | ✅ 有时有意义 | gzip + bz2、tar + xz 等，但属于特定场景 |
| 用于**多媒体压缩格式**（如 JPEG、MP3） | ❌ 无意义     | 它们是有损压缩，已压缩到底，无法再压    |

🛠️ 特例：极少数多级压缩格式

有些高效压缩格式会使用多级算法组合（比如 `.tar.gz.bz2.xz`）——但那是**不同算法协作**，每层压缩处理不同类型的冗余（结构性冗余、字典冗余、熵编码）。

Snappy、LZ4、Zlib 这类 fast compression 都是单层轻量压缩，**没有必要递归使用自己**。

### 2.3压缩算法的选择

目前市面上**开源压缩算法中压缩比率最好的（无损压缩）算法**，主要是在**通用压缩场景**下进行优化的，比如文本、大量日志、数据库快照等。下面是一些知名算法及其典型压缩比率表现：

------

📊 常见开源压缩算法压缩比对比（文本/日志/源代码类数据）

| 算法名称             | 压缩比（典型范围）             | 压缩速度         | 解压速度 | 特点                           |
| -------------------- | ------------------------------ | ---------------- | -------- | ------------------------------ |
| **zlib/DEFLATE**     | 2x–3x                          | 中等             | 快       | 老牌通用压缩，gzip 用的就是它  |
| **bzip2**            | 2.5x–4x                        | 慢               | 中等     | 更高压缩比但慢，适合冷数据     |
| **LZMA/xz**          | 3x–5x（文本可到7x）            | 慢               | 慢       | 高压缩比，7-zip 使用它         |
| **Zstandard (zstd)** | 2x–4x（高压缩级别可达 6x）     | 快（调节可平衡） | 非常快   | Facebook 出品，现代推荐        |
| **Brotli**           | 2.5x–4x（网页文本压缩可达 6x） | 中等             | 快       | 主要用于网页传输，如 HTTP      |
| **zpaq**             | 4x–10x（极端场景）             | 非常慢           | 非常慢   | 极高压缩比，适合归档和科研     |
| **CMIX**             | 可达 10x–15x+（文本极限）      | 极其慢（几小时） | 极其慢   | 基于人工智能模型预测，研究用途 |

------

✅ 更清晰的对比（以 100MB 文本数据为例）：

| 压缩算法  | 压缩后体积 | 压缩耗时       | 解压耗时 |
| --------- | ---------- | -------------- | -------- |
| gzip/zlib | ~30–50MB   | 快             | 快       |
| xz        | ~20–35MB   | 慢             | 中       |
| zstd      | ~25–40MB   | 非常快（可调） | 非常快   |
| brotli    | ~25–35MB   | 中等           | 快       |
| zpaq/cmix | ~5–15MB    | 非常慢         | 非常慢   |

> 注意：压缩比高度依赖原始数据类型。例如纯文本压缩比极高（可达 10x），但视频/图像（如 JPEG）等已压缩数据几乎无法再压缩。

------

🎯 总结推荐

| 目标                  | 推荐算法             | 理由                                |
| --------------------- | -------------------- | ----------------------------------- |
| 高压缩比（归档/备份） | **xz / zstd –ultra** | 平衡压缩比和速度                    |
| 高速压缩与快速解压    | **Zstandard (zstd)** | Facebook 出品，工业级强推           |
| 极限压缩（科研/演示） | **zpaq / cmix**      | 实验性质，不推荐用于生产            |
| 网页传输              | **Brotli**           | HTTP2 支持、压缩 HTML/CSS/JS 效果好 |

------

如果你愿意提供具体的使用场景（如日志压缩、冷存归档、传输优化、固件体积最小化等），我可以给出更合适的压缩算法和参数推荐。需要吗？

## 3.文件存在性算法（Bloom filters ）

### 3.1 原理

​	Bloom filter 基于哈希函数实现。它初始化时是一个大小为 m 的位阵列，所有位都设置为 0。有 k 个不同的哈希函数，每个函数都能将元素映射到位阵列的某个索引（范围是 0 到 m - 1）。当插入一个元素时，该元素会通过这 k 个哈希函数计算出 k 个索引位置，并将这些位置的位设置为 1。查询元素时，同样通过 k 个哈希函数计算索引，如果所有对应位置的位都是 1，则表示元素可能在集合中；若有任何一个位置的位是 0，则可确定元素一定不在集合中。

### 3.2 优势与权衡

优势是能快速给出元素是否在集合中的近似答案，适用于处理大型数据集，可显著减少磁盘寻道等昂贵操作。不过，它是一种概率型数据结构，存在误判可能，即会**有一定的假阳性率**，可能将不在集合中的元素误判为在集合中，但**不会出现假阴性**（即不会将在集合中的元素误判为不在集合中）。此外，为了快速查询，Bloom filter 需要存储在内存中，这会占用一定的内存空间，但通常这种内存占用换来的性能提升是值得的。

# 五、未归类

## 1.NFS 网络文件系统

### 1.1 NFS的dircache的作用

✅ 你的代码核心逻辑总结

在 `nfs3_opendir_continue_internal()` 函数中：

1. 如果发现缓存（`nfs_dircache_find` 找到 `cached`）：
   - 对比 `attr->mtime` 是否与缓存中的 `cached->attr.mtime` 相同；
   - 如果一致，说明缓存仍然有效，直接用缓存数据并回调；
   - 否则，认为缓存过期，释放旧缓存，重新从服务器获取目录内容。

------

❓ 疑问重述：**既然每次都 stat，为什么还要缓存？**

这是一个非常有代表性的问题，答案主要包含以下几点：

------

1. ⚡ 只 stat 而不 readdirplus：减少带宽和 IOPS 成本

虽然每次都会进行一次属性请求（`GETATTR`），但如果 `mtime` 没变，就可以：

- **避免整个目录内容的重新获取（`READDIRPLUS`）**；
- 避免重新构造目录条目链表；
- 避免每次目录打开都执行网络 I/O 获取所有 entry；

👉 所以，**缓存真正缓存的是 `READDIRPLUS` 的返回值（即目录内容）**，**不是目录属性本身**。
 `mtime` 只是一个用于验证缓存是否失效的快速属性。

📌 举个例子：

```c
ls /mnt/nfs/some_dir
```

- 如果 `mtime` 没变，则可以复用缓存中的目录项，无需发送 `READDIRPLUS` 请求；
- 否则再触发完整拉取。

------

2. ⏱ 属性请求（GETATTR）远比 readdirplus 轻量

- `GETATTR`（即 stat）是非常轻量级的 RPC 请求；
- `READDIRPLUS` 不仅读取目录，还要拉取其中每个 entry 的文件属性（是一个非常重的 RPC 操作）；
- 因此，用 `GETATTR` 来判断是否 invalid cache 是一种高性价比做法。

📌 即使 stat 一下服务器，也能节省大量数据传输和延迟，尤其是目录内容特别多的情况下。

------

3. 🔁 缓存用于频繁访问目录（尤其在 GUI 或脚本中）

- 很多图形界面文件浏览器、shell 脚本会频繁打开目录（如刷新目录、点击目录项）；
- 如果不带缓存，每次都 readdirplus，会非常慢；
- 有了缓存，即使 stat 一下，只要 mtime 没变，就可以复用原缓存，**避免重复访问同一目录结构**。

------

4. ✅ 缓存策略可以调节（你可以禁用或弱化属性校验）

在 `libnfs` 或内核 NFS 客户端中，用户可以通过挂载参数调节缓存行为：

- `acdirmin/acdirmax`：控制目录属性缓存的有效期；
- `noac`：完全禁用缓存（强一致性但低性能）；
- 或者在应用层增加缓存保留时间、懒校验策略（不是每次都检查 mtime）。

------

📌 小结

| 组件           | 内容                                | 是否缓存           | 缓存的意义                      |
| -------------- | ----------------------------------- | ------------------ | ------------------------------- |
| `GETATTR`      | 目录的 `mtime`, `ctime` 等属性      | 不缓存（每次调用） | 作为目录缓存有效性的验证手段    |
| `READDIRPLUS`  | 目录中的所有 entry 及其属性         | **缓存**           | 一旦 mtime 没变，复用，避免拉取 |
| `nfs_dircache` | 缓存结构体，保存 readdirplus 的结果 | 是                 | 用于快速返回目录结构            |

所以结论是：

> ✅ **缓存的是目录内容（READDIRPLUS 结果）**，不是 mtime；而 mtime 只是校验用。即便每次都 stat，仍然能通过避免 readdirplus 获得显著性能收益。

1.2 NFS中的pagecache原理:

​	libnfs.c文件中 nfs_dircache_add， nfs_dircache_find， nfs_dircache_drop等函数分析一下
